{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bert_multitask_robustness",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Note: Get the data from the github repo: https://github.com/Georgepu1/cs263-final-project/\n",
        "\n",
        "- Good resource: https://colab.research.google.com/github/zphang/zphang.github.io/blob/master/files/notebooks/Multi_task_Training_with_Transformers_NLP.ipynb#scrollTo=LlICaYzQan59\n",
        "\n"
      ],
      "metadata": {
        "id": "JQa_KeE1krF7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install torchtext"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rzS-LJfRxrXX",
        "outputId": "3638d307-4a6b-44af-a75d-b12011684799"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.19.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.5.18.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchtext in /usr/local/lib/python3.7/dist-packages (0.12.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext) (2.23.0)\n",
            "Requirement already satisfied: torch==1.11.0 in /usr/local/lib/python3.7/dist-packages (from torchtext) (1.11.0+cu113)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext) (4.64.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.11.0->torchtext) (4.2.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext) (2022.5.18.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext) (2.10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Note first get the data from the github to start data prep phase\n",
        "import pandas as pd\n",
        "\n",
        "sst_tr_dataset = pd.read_csv('sst_train_data.csv')\n",
        "sst_val_dataset = pd.read_csv('sst_val_data.csv')\n",
        "sst_test_dataset = pd.read_csv('sst_test_data.csv')\n",
        "\n",
        "cola_tr_dataset = pd.read_csv('cola_train_data.csv')\n",
        "cola_val_dataset = pd.read_csv('cola_val_data.csv')\n",
        "cola_test_dataset = pd.read_csv('cola_test_data.csv')\n",
        "\n",
        "sst_tr_dataset.sample(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "ZUF62VszdFaa",
        "outputId": "a43dad1c-405a-4761-c762-8b672b39ab71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                              sentences  labels\n",
              "6542  A big meal of cliches that the talented cast g...       0\n",
              "1714                 Films are made of little moments .       1\n",
              "7101  Chelsea Walls is a case of too many chefs fuss...       0\n",
              "7210  ( Jackson and Bledel ) seem to have been picke...       0\n",
              "5597  Despite the pyrotechnics , Narc is strictly by...       0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b79e7782-c768-4d0f-9094-2283c63ca2e0\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentences</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>6542</th>\n",
              "      <td>A big meal of cliches that the talented cast g...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1714</th>\n",
              "      <td>Films are made of little moments .</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7101</th>\n",
              "      <td>Chelsea Walls is a case of too many chefs fuss...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7210</th>\n",
              "      <td>( Jackson and Bledel ) seem to have been picke...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5597</th>\n",
              "      <td>Despite the pyrotechnics , Narc is strictly by...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b79e7782-c768-4d0f-9094-2283c63ca2e0')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b79e7782-c768-4d0f-9094-2283c63ca2e0 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b79e7782-c768-4d0f-9094-2283c63ca2e0');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "\n",
        "model_name = \"bert-base-uncased\"\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "fVu22_V_gb01"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import random\n",
        "import transformers\n",
        "from transformers import pipeline, AutoTokenizer, DataCollatorWithPadding, AutoModelForSequenceClassification, TrainingArguments, Trainer, AutoConfig\n",
        "from transformers import AutoConfig, AutoModelForPreTraining\n",
        "from transformers import AutoTokenizer, AdamW, get_linear_schedule_with_warmup\n",
        "from collections import namedtuple\n"
      ],
      "metadata": {
        "id": "8akKpsGtlAdv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import namedtuple\n",
        "\n",
        "gen_batch_fields = ['sst_input_text', 'sst_id_text', 'sst_attention_mask', 'sst_target', 'cola_input_text', 'cola_id_text', 'cola_attention_mask', 'cola_target']\n",
        "GenBatch = namedtuple('GenBatch', field_names=gen_batch_fields, defaults=[None] * len(gen_batch_fields))"
      ],
      "metadata": {
        "id": "VrgjGQHVReXo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset for LSTM + Embedding / BERT\n",
        "class MultitaskDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, tokenizer, sst_X, sst_y, cola_X, cola_y, max_len=10000, max_output_length=64):\n",
        "    assert len(sst_X) == len(sst_y), \"SST data not one to one\"\n",
        "    assert len(cola_X) == len(cola_y), \"CoLA data not one to one\"\n",
        "\n",
        "    self.max_len = max_len\n",
        "    self.tokenizer = tokenizer\n",
        "    self.max_output_length = max_output_length\n",
        "    min_samples = min(len(sst_X), len(cola_X))\n",
        "    self.data = []\n",
        "    sst_X = sst_X[:min(min_samples, max_len)]\n",
        "    sst_y = sst_y[:min(min_samples, max_len)]\n",
        "    cola_X = cola_X[:min(min_samples, max_len)]\n",
        "    cola_y = cola_y[:min(min_samples, max_len)]\n",
        "    self.load_data(sst_X, sst_y, cola_X, cola_y)\n",
        "\n",
        "  def load_data(self, sst_X, sst_y, cola_X, cola_y):\n",
        "    for (cur_sst_X, cur_sst_y, cur_cola_X, cur_cola_y) in zip(sst_X, sst_y, cola_X, cola_y):\n",
        "      self.data.append({\n",
        "          'sst_input': cur_sst_X, \n",
        "          'sst_target': cur_sst_y, \n",
        "          'cola_input': cur_cola_X, \n",
        "          'cola_target': cur_cola_y,\n",
        "      })\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "      # Get element consisting of sst_X, sst_y, cola_X, and cola_y\n",
        "    return self.data[index]\n",
        "  \n",
        "  def collate_fn(self, batch):\n",
        "    sst_input_text = [x['sst_input'] for x in batch]\n",
        "    sst_inputs = self.tokenizer(sst_input_text, return_tensors=\"pt\", padding=True, max_length=self.max_output_length)\n",
        "    cola_input_text = [x['cola_input'] for x in batch]\n",
        "    cola_inputs = self.tokenizer(cola_input_text, return_tensors=\"pt\", padding=True, max_length=self.max_output_length)\n",
        "\n",
        "    return GenBatch(\n",
        "      sst_input_text=sst_input_text,\n",
        "      sst_id_text=sst_inputs['input_ids'],\n",
        "      sst_attention_mask=sst_inputs['attention_mask'],\n",
        "      sst_target=[x['sst_target'] for x in batch],\n",
        "      cola_input_text=cola_input_text,\n",
        "      cola_id_text=cola_inputs['input_ids'],\n",
        "      cola_attention_mask=cola_inputs['attention_mask'],\n",
        "      cola_target=[x['cola_target'] for x in batch]\n",
        "    )"
      ],
      "metadata": {
        "id": "8LkFzD6dPBRu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_set = MultitaskDataset(tokenizer, sst_tr_dataset['sentences'], sst_tr_dataset['labels'], \n",
        "                             cola_tr_dataset['sentences'], cola_tr_dataset['labels'])\n",
        "val_set = MultitaskDataset(tokenizer, sst_val_dataset['sentences'], sst_val_dataset['labels'], \n",
        "                           cola_val_dataset['sentences'], cola_val_dataset['labels'])\n",
        "test_set = MultitaskDataset(tokenizer, sst_test_dataset['sentences'], sst_test_dataset['labels'], \n",
        "                             cola_test_dataset['sentences'], cola_test_dataset['labels'])\n",
        "train_loader = DataLoader(train_set, shuffle=True, batch_size=8,\n",
        "                          collate_fn=train_set.collate_fn)\n",
        "val_loader  = DataLoader(val_set, shuffle=False, batch_size=8,\n",
        "                          collate_fn=val_set.collate_fn)\n",
        "test_loader  = DataLoader(test_set, shuffle=False, batch_size=8,\n",
        "                          collate_fn=test_set.collate_fn)\n"
      ],
      "metadata": {
        "id": "wSLscrbcPFFX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\", padding=True, max_length=64)\n",
        "# inputs.keys()\n",
        "# outputs = model(**inputs)\n",
        "\n",
        "# last_hidden_states = outputs.last_hidden_state"
      ],
      "metadata": {
        "id": "XveNwXEVPFUT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "class MultitaskBert(torch.nn.Module):\n",
        "  def __init__(self, model_name, tokenizer, num_labels=2, hidden_size=768):\n",
        "    super().__init__()\n",
        "\n",
        "    self.num_labels = num_labels\n",
        "    self.tokenizer = tokenizer\n",
        "    self.bert = BertModel.from_pretrained(model_name)\n",
        "    # self.dropout = nn.Dropout(.1)\n",
        "    self.classifier = nn.ModuleList([nn.Linear(hidden_size, self.num_labels) for i in range(2)])\n",
        "    self.init_weights()\n",
        "\n",
        "  def init_weights(self):\n",
        "    initrange = 0.5\n",
        "    self.classifier[0].weight.data.uniform_(-initrange, initrange)\n",
        "    self.classifier[1].weight.data.uniform_(-initrange, initrange)\n",
        "    self.classifier[0].bias.data.zero_()\n",
        "    self.classifier[1].bias.data.zero_()\n",
        "\n",
        "  def forward(self, input_ids, attention_mask, ind):\n",
        "    outputs = self.bert(\n",
        "      input_ids,\n",
        "      attention_mask=attention_mask\n",
        "    )\n",
        "\n",
        "    pooled_output = outputs[1]\n",
        "    # Note BERT Model already applies dropout in output\n",
        "    # pooled_output = self.dropout(pooled_output)\n",
        "    logits = self.classifier[ind](pooled_output)\n",
        "    # print(logits.shape)\n",
        "    return F.softmax(logits, dim=-1)"
      ],
      "metadata": {
        "id": "cURqYBG4uD4F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AdamW\n",
        "\n",
        "model = MultitaskBert(model_name, tokenizer)\n",
        "optimizer = AdamW(model.parameters())\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "def eval_metrics(model, dl, criterion):\n",
        "  model.eval()\n",
        "  sst_acc = 0\n",
        "  sst_loss = 0\n",
        "  cola_acc = 0\n",
        "  cola_loss = 0\n",
        "  total = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for sample in dl:\n",
        "      y1_pred = model(sample.sst_id_text, sample.sst_attention_mask, 0).reshape(-1, 2)\n",
        "      y2_pred = model(sample.cola_id_text, sample.cola_attention_mask, 1).reshape(-1, 2)\n",
        "      sample_sst_target = torch.tensor(sample.sst_target).type(torch.long).reshape(-1)\n",
        "      sample_cola_target = torch.tensor(sample.cola_target).type(torch.long).reshape(-1)\n",
        "\n",
        "      sst_loss = criterion(y1_pred, sample_sst_target)\n",
        "      cola_loss = criterion(y2_pred, sample_cola_target)\n",
        "      \n",
        "      # print('Length: {}'.format(y1_pred.shape[0]))\n",
        "      sst_acc += (torch.eq(sample_sst_target,y1_pred.argmax(1))).sum().item()\n",
        "      cola_acc += (torch.eq(sample_cola_target, y2_pred.argmax(1))).sum().item()\n",
        "      sst_loss += sst_loss.item()\n",
        "      cola_loss += cola_loss.item()\n",
        "      total += y1_pred.shape[0]\n",
        "\n",
        "  return sst_acc/total, sst_loss/total, cola_acc/total, cola_loss/total"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kTJjZH3psRul",
        "outputId": "19ce5f1f-4044-4636-8b8b-c6923395e142"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# m = nn.Sigmoid()\n",
        "# loss = nn.BCELoss()\n",
        "# input = torch.randn(3, requires_grad=True)\n",
        "# target = torch.empty(3).random_(2)\n",
        "\n",
        "# output = loss(m(input), target)\n",
        "# output.backward()\n",
        "# print(input.shape, target.shape)"
      ],
      "metadata": {
        "id": "ooTbWIuqV8Wr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "epochs = 10\n",
        "\n",
        "for _ in range(epochs):\n",
        "  \n",
        "  tr_count = 0\n",
        "  sst_tr_acc = 0\n",
        "  sst_tr_loss = 0\n",
        "  cola_tr_acc = 0\n",
        "  cola_tr_loss = 0\n",
        "  \n",
        "  model.train()\n",
        "  for sample in tqdm(train_loader):\n",
        "    # Note can also set the data to a decide (cuda)\n",
        "    model.zero_grad()\n",
        "    \n",
        "    y1_pred = model(sample.sst_id_text, sample.sst_attention_mask, 0).reshape(-1, 2)\n",
        "    y2_pred = model(sample.cola_id_text, sample.cola_attention_mask, 1).reshape(-1, 2)\n",
        "    sample_sst_target = torch.tensor(sample.sst_target).type(torch.long).reshape(-1)\n",
        "    sample_cola_target = torch.tensor(sample.cola_target).type(torch.long).reshape(-1)\n",
        "\n",
        "    sst_loss = criterion(y1_pred, sample_sst_target)\n",
        "    cola_loss = criterion(y2_pred, sample_cola_target)\n",
        "    # multitask loss\n",
        "    loss = sst_loss + cola_loss\n",
        "    loss.backward()\n",
        "    # intermediate variabels stores embedding of x and computes\n",
        "    # m_output.grad w.r.t. this and calculate the MSE of the m_output.grad(emb_x)\n",
        "    # norm and 1.0 and use relu on; before you do the square, pass it through a relu\n",
        "    # so everything les than 1.0 wont be counted to the square.\n",
        "    # MSE (m_output.grad(x), 1.0) calulate gradient of M output w.r.t. x's embedding space\n",
        "    # To avoid overfitting, calculating regularization term can use a varied version of x\n",
        "    # instead of the original (e.g. add gaussian noise around embeddings of x); can\n",
        "    # also minimize discrepancy on the two for robustness of model\n",
        "    # Lipschitz-regularized loss\n",
        "    optimizer.step()\n",
        "    # print('Length: {}'.format(y1_pred.shape[0]))\n",
        "    sst_tr_acc += (torch.eq(sample_sst_target,y1_pred.argmax(1))).sum().item()\n",
        "    cola_tr_acc += (torch.eq(sample_cola_target, y2_pred.argmax(1))).sum().item()\n",
        "    sst_tr_loss += sst_loss.item()\n",
        "    cola_tr_loss += cola_loss.item()\n",
        "    tr_count += y1_pred.shape[0]\n",
        "    \n",
        "  print(\"SST Train accuracy: {}, CoLA Train accuracy: {}, SST Train loss: {}, CoLA Train loss: {}\".format(sst_tr_acc/tr_count, cola_tr_acc/tr_count, sst_tr_loss/tr_count, cola_tr_loss/tr_count))\n",
        "  val_sst_acc, val_sst_loss, val_cola_acc, val_cola_loss = eval_metrics(model, val_loader, criterion)\n",
        "  print(\"SST Val accuracy: {}, CoLA Val accuracy: {}, SST Val loss: {}, CoLA Val loss: {} \".format(val_sst_acc, val_cola_acc, val_sst_loss, val_cola_loss))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "id": "59hUPeePQ3LA",
        "outputId": "360c52a9-7aa0-466f-81b1-535009f4287e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/962 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2302: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  \"`max_length` is ignored when `padding`=`True` and there is no truncation strategy. \"\n",
            "  0%|          | 2/962 [00:15<2:04:03,  7.75s/it]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-121-370e91c922bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;31m# multitask loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msst_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcola_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0;31m# intermediate variabels stores embedding of x and computes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;31m# m_output.grad w.r.t. this and calculate the MSE of the m_output.grad(emb_x)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    361\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    173\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m def grad(\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# learning_rate = 3e-4\n",
        "# criterion = nn.BCELoss()\n",
        "# # optimizer = torch.optim.SGD(model.parameters(),lr=learning_rate)\n",
        "# optimizer = torch.optim.Adam(model.parameters(),lr=learning_rate)\n",
        "# offset_input = torch.tensor([0])\n"
      ],
      "metadata": {
        "id": "hssLI9PLrh0X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_sst_acc, test_sst_loss, test_cola_acc, test_cola_loss = eval_metrics(model, val_loader, criterion)\n",
        "print(\"SST Test accuracy: {}, CoLA Val accuracy: {}, SST Val loss: {}, CoLA Val loss: {} \".format(test_sst_acc, test_cola_acc, test_sst_loss, test_cola_loss))"
      ],
      "metadata": {
        "id": "MD_7Nktarh29",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7d35e97-1a9d-4b3d-84b4-999df49f5478"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2302: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  \"`max_length` is ignored when `padding`=`True` and there is no truncation strategy. \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SST Test accuracy: 0.39369158878504673, CoLA Val accuracy: 0.2850467289719626, SST Val loss: 0.0007324839825741947, CoLA Val loss: 0.0018050861544907093 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sst_tr_dataset.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "p4oF5A-Nk52F",
        "outputId": "c6e76af5-0916-440e-bd46-a98fb0485823"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                           sentences  labels\n",
              "0  The Rock is destined to be the 21st Century 's...       1\n",
              "1  The gorgeously elaborate continuation of `` Th...       1\n",
              "2  Singer\\/composer Bryan Adams contributes a sle...       1\n",
              "3  You 'd think by now America would have had eno...       0\n",
              "4               Yet the act is still charming here .       1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-76ab2ebe-b955-49b1-ab3b-3d9229007d18\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentences</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>The Rock is destined to be the 21st Century 's...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>The gorgeously elaborate continuation of `` Th...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Singer\\/composer Bryan Adams contributes a sle...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>You 'd think by now America would have had eno...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Yet the act is still charming here .</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-76ab2ebe-b955-49b1-ab3b-3d9229007d18')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-76ab2ebe-b955-49b1-ab3b-3d9229007d18 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-76ab2ebe-b955-49b1-ab3b-3d9229007d18');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_sst_acc, test_sst_loss, test_cola_acc, test_cola_loss = eval_metrics(model, val_loader, criterion)\n",
        "print(\"SST Test accuracy: {}, CoLA Val accuracy: {}, SST Val loss: {}, CoLA Val loss: {} \".format(test_sst_acc, test_cola_acc, test_sst_loss, test_cola_loss))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JeT2uQEIhS8n",
        "outputId": "68fec03a-3579-47f7-99f0-bc06ba075146"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SST Test accuracy: 0.6063084112149533, CoLA Val accuracy: 0.6273364485981309, SST Val loss: 0.23364485800266266, CoLA Val loss: 0.0034176348708570004 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TODO: Adversarial attacks and Robustness Evaluation"
      ],
      "metadata": {
        "id": "NlCTcEOOSgcV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def attack():\n",
        "  pass"
      ],
      "metadata": {
        "id": "fLH_bFIASkWE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}