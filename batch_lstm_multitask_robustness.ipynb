{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lstm_multitask_robustness",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Note: Get the data from the github repo: https://github.com/Georgepu1/cs263-final-project/"
      ],
      "metadata": {
        "id": "JQa_KeE1krF7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install torchtext"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rzS-LJfRxrXX",
        "outputId": "5e1ed602-c71f-4c5c-9950-e24b4db0be1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.19.2-py3-none-any.whl (4.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.2 MB 27.0 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.7.0-py3-none-any.whl (86 kB)\n",
            "\u001b[K     |████████████████████████████████| 86 kB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 51.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 40.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.5.18.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Installing collected packages: pyyaml, tokenizers, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.7.0 pyyaml-6.0 tokenizers-0.12.1 transformers-4.19.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchtext in /usr/local/lib/python3.7/dist-packages (0.12.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext) (4.64.0)\n",
            "Requirement already satisfied: torch==1.11.0 in /usr/local/lib/python3.7/dist-packages (from torchtext) (1.11.0+cu113)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.11.0->torchtext) (4.2.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext) (2022.5.18.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Note first get the data from the github to start data prep phase\n",
        "import pandas as pd\n",
        "\n",
        "sst_tr_dataset = pd.read_csv('sst_train_data.csv')\n",
        "sst_val_dataset = pd.read_csv('sst_val_data.csv')\n",
        "sst_test_dataset = pd.read_csv('sst_test_data.csv')\n",
        "\n",
        "cola_tr_dataset = pd.read_csv('cola_train_data.csv')\n",
        "cola_val_dataset = pd.read_csv('cola_val_data.csv')\n",
        "cola_test_dataset = pd.read_csv('cola_test_data.csv')\n",
        "\n",
        "sst_tr_dataset.sample(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "ZUF62VszdFaa",
        "outputId": "6c6bd446-bdf8-4ef5-85d2-139d03beeb98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                              sentences  labels\n",
              "7681  The connected stories of Breitbart and Hanusse...       0\n",
              "3008  But arriving at a particularly dark moment in ...       1\n",
              "965   Majidi 's direction has never been smoother or...       1\n",
              "27    They are what makes it worth the trip to the t...       1\n",
              "4933  There 's already been too many of these films ...       0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4425a2d3-ad62-4c61-bbd3-1a33e019bf66\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentences</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>7681</th>\n",
              "      <td>The connected stories of Breitbart and Hanusse...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3008</th>\n",
              "      <td>But arriving at a particularly dark moment in ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>965</th>\n",
              "      <td>Majidi 's direction has never been smoother or...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>They are what makes it worth the trip to the t...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4933</th>\n",
              "      <td>There 's already been too many of these films ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4425a2d3-ad62-4c61-bbd3-1a33e019bf66')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-4425a2d3-ad62-4c61-bbd3-1a33e019bf66 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-4425a2d3-ad62-4c61-bbd3-1a33e019bf66');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.vocab import GloVe, vocab\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "\n",
        "global_vectors = GloVe(name='6B', dim=300)\n",
        "tokenizer = get_tokenizer(\"basic_english\")"
      ],
      "metadata": {
        "id": "D19W47LJgJV3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import random\n",
        "import transformers\n",
        "# from transformers import pipeline, AutoTokenizer, DataCollatorWithPadding, AutoModelForSequenceClassification, TrainingArguments, Trainer, AutoConfig\n",
        "\n",
        "\n",
        "glove_vocab = vocab(global_vectors.stoi)\n",
        "unk_token = \"<unk>\"\n",
        "unk_index = 0\n",
        "glove_vocab.insert_token(\"<unk>\", unk_index)\n",
        "glove_vocab.set_default_index(unk_index)\n",
        "pretrained_embeddings = global_vectors.vectors\n",
        "pretrained_embeddings = torch.cat((torch.zeros(1,pretrained_embeddings.shape[1]),pretrained_embeddings))"
      ],
      "metadata": {
        "id": "8akKpsGtlAdv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def tokenize_sentences(x, tokenizer, max_words = 48):\n",
        "#   x = tokenizer(x)\n",
        "#   if len(x) < max_words:\n",
        "#     x = x + [\"\"] * (max_words - len(x))\n",
        "#   else:\n",
        "#     x = x[:max_words]\n",
        "\n",
        "#   return glove_vocab(x)\n",
        "\n",
        "# sst_tr_dataset['sentences'] = sst_tr_dataset['sentences'].apply(lambda x: tokenize_sentences(x, tokenizer))\n",
        "# sst_val_dataset['sentences'] = sst_val_dataset['sentences'].apply(lambda x: tokenize_sentences(x, tokenizer))\n",
        "# sst_test_dataset['sentences'] = sst_test_dataset['sentences'].apply(lambda x: tokenize_sentences(x, tokenizer))\n",
        "# cola_tr_dataset['sentences'] = cola_tr_dataset['sentences'].apply(lambda x: tokenize_sentences(x, tokenizer))\n",
        "# cola_val_dataset['sentences'] = cola_val_dataset['sentences'].apply(lambda x: tokenize_sentences(x, tokenizer))\n",
        "# cola_test_dataset['sentences'] = cola_test_dataset['sentences'].apply(lambda x: tokenize_sentences(x, tokenizer))\n"
      ],
      "metadata": {
        "id": "SYD-6cPLx_Nb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import namedtuple\n",
        "\n",
        "gen_batch_fields = ['sst_input_text', 'sst_input_inds', 'sst_target', 'cola_input_text', 'cola_input_inds', 'cola_target']\n",
        "GenBatch = namedtuple('GenBatch', field_names=gen_batch_fields, defaults=[None] * len(gen_batch_fields))"
      ],
      "metadata": {
        "id": "Fl5_DEndP-1r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset for LSTM + Embedding / BERT\n",
        "class MultitaskDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, vocab, tokenizer, sst_X, sst_y, cola_X, cola_y, max_len=10000):\n",
        "    assert len(sst_X) == len(sst_y), \"SST data not one to one\"\n",
        "    assert len(cola_X) == len(cola_y), \"CoLA data not one to one\"\n",
        "\n",
        "    self.max_len = max_len\n",
        "    self.tokenizer = tokenizer\n",
        "    self.vocab = vocab\n",
        "    self.data = []\n",
        "    min_samples = min(len(sst_X), len(cola_X))\n",
        "    sst_X = sst_X[:min(min_samples, max_len)]\n",
        "    sst_y = sst_y[:min(min_samples, max_len)]\n",
        "    cola_X = cola_X[:min(min_samples, max_len)]\n",
        "    cola_y = cola_y[:min(min_samples, max_len)]\n",
        "    self.load_data(sst_X, sst_y, cola_X, cola_y)\n",
        "\n",
        "  def load_data(self, sst_X, sst_y, cola_X, cola_y):\n",
        "    for (cur_sst_X, cur_sst_y, cur_cola_X, cur_cola_y) in zip(sst_X, sst_y, cola_X, cola_y):\n",
        "      self.data.append({\n",
        "          'sst_input': cur_sst_X, \n",
        "          'sst_target': cur_sst_y, \n",
        "          'cola_input': cur_cola_X, \n",
        "          'cola_target': cur_cola_y,\n",
        "      })\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "      # Get element consisting of sst_X, sst_y, cola_X, and cola_y\n",
        "    return self.data[index]\n",
        "\n",
        "  def tokenize_sentences(self, x, max_words=32):\n",
        "    x = self.tokenizer(x)\n",
        "    if len(x) < max_words:\n",
        "      x = x + [\"\"] * (max_words - len(x))\n",
        "    else:\n",
        "      x = x[:max_words]\n",
        "    return self.vocab(x)\n",
        "    \n",
        "  def collate_fn(self, batch):\n",
        "    sst_input_text = [x['sst_input'] for x in batch]\n",
        "    sst_inputs = [self.tokenize_sentences(x['sst_input']) for x in batch]\n",
        "    cola_input_text = [x['cola_input'] for x in batch]\n",
        "    cola_inputs = [self.tokenize_sentences(x['cola_input']) for x in batch]\n",
        "\n",
        "    return GenBatch(\n",
        "      sst_input_text=sst_input_text,\n",
        "      sst_input_inds=sst_inputs,\n",
        "      sst_target=[x['sst_target'] for x in batch],\n",
        "      cola_input_text=cola_input_text,\n",
        "      cola_input_inds=cola_inputs,\n",
        "      cola_target=[x['cola_target'] for x in batch]\n",
        "    )"
      ],
      "metadata": {
        "id": "43jThE2gHzCc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_set = MultitaskDataset(glove_vocab, tokenizer, sst_tr_dataset['sentences'], sst_tr_dataset['labels'], \n",
        "                             cola_tr_dataset['sentences'], cola_tr_dataset['labels'])\n",
        "val_set = MultitaskDataset(glove_vocab, tokenizer, sst_val_dataset['sentences'], sst_val_dataset['labels'], \n",
        "                           cola_val_dataset['sentences'], cola_val_dataset['labels'])\n",
        "test_set = MultitaskDataset(glove_vocab, tokenizer, sst_test_dataset['sentences'], sst_test_dataset['labels'], \n",
        "                             cola_test_dataset['sentences'], cola_test_dataset['labels'])"
      ],
      "metadata": {
        "id": "W53PhWXhHEVm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(train_set, shuffle=True, batch_size=8,\n",
        "                          collate_fn=train_set.collate_fn)\n",
        "val_loader  = DataLoader(val_set, shuffle=False, batch_size=8,\n",
        "                          collate_fn=val_set.collate_fn)\n",
        "test_loader  = DataLoader(test_set, shuffle=False, batch_size=8,\n",
        "                          collate_fn=test_set.collate_fn)"
      ],
      "metadata": {
        "id": "Jflla-4EIJ-9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "class MultiTaskLSTM(torch.nn.Module):\n",
        "  def __init__(self, pretrain_emb, emb_dim, hidden_dim):\n",
        "    super().__init__()\n",
        "    self.embeddings = nn.EmbeddingBag.from_pretrained(pretrain_emb)\n",
        "    self.lstm = nn.LSTM(emb_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
        "    self.linears = nn.ModuleList([nn.Linear(hidden_dim, 2) for i in range(2)])\n",
        "    self.dropout = nn.Dropout(0.2)\n",
        "    self.init_weights()\n",
        "\n",
        "  def init_weights(self):\n",
        "    initrange = 0.5\n",
        "    self.linears[0].weight.data.uniform_(-initrange, initrange)\n",
        "    self.linears[1].weight.data.uniform_(-initrange, initrange)\n",
        "    self.linears[0].bias.data.zero_()\n",
        "    self.linears[1].bias.data.zero_()\n",
        "\n",
        "  def forward(self, x, ind, offset):\n",
        "    print('inside forward')\n",
        "    print(x.shape)\n",
        "    x = self.embeddings(x, offset)\n",
        "    print(x.shape)\n",
        "    x = self.dropout(x)\n",
        "    print(x.shape)\n",
        "    print('done with dropout')\n",
        "    # x_pack = pack_padded_sequence(x, s, batch_first=True, enforce_sorted=False)\n",
        "    lstm_out, (ht, ct) = self.lstm(x)\n",
        "    logits = self.linears[ind](ht[-1])\n",
        "\n",
        "    return F.softmax(logits, dim=-1)\n",
        "\n",
        "model = MultiTaskLSTM(pretrained_embeddings, 300, 64)"
      ],
      "metadata": {
        "id": "C342rTA_O8mL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 3e-4\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# optimizer = torch.optim.SGD(model.parameters(),lr=learning_rate)\n",
        "# TODO: maybe need to truncate and clip gradient\n",
        "optimizer = torch.optim.Adam(model.parameters(),lr=learning_rate)\n",
        "\n",
        "def eval_metrics(model, dl, criterion, offset_input):\n",
        "  model.eval()\n",
        "  sst_acc = 0\n",
        "  sst_loss = 0\n",
        "  cola_acc = 0\n",
        "  cola_loss = 0\n",
        "  total = 0\n",
        "  for sample in dl:\n",
        "    y1_pred = model(sample.sst_input_inds, 0, offset_input).reshape(-1, 2)\n",
        "    y2_pred = model(sample.cola_input_inds, 1, offset_input).reshape(-1, 2)\n",
        "    sample_sst_target = torch.tensor(sample.sst_target).type(torch.long).reshape(-1)\n",
        "    sample_cola_target = torch.tensor(sample.cola_target).type(torch.long).reshape(-1)\n",
        "\n",
        "    sst_loss = criterion(y1_pred, sample_sst_target)\n",
        "    cola_loss = criterion(y2_pred, sample_cola_target)\n",
        "    \n",
        "    # print('Length: {}'.format(y1_pred.shape[0]))\n",
        "    sst_acc += (torch.eq(sample_sst_target,y1_pred.argmax(1))).sum().item()\n",
        "    cola_acc += (torch.eq(sample_cola_target, y2_pred.argmax(1))).sum().item()\n",
        "    sst_loss += sst_loss.item()\n",
        "    cola_loss += cola_loss.item()\n",
        "    total += y1_pred.shape[0]\n",
        "\n",
        "  return sst_acc/total, sst_loss/total, cola_acc/total, cola_loss/total"
      ],
      "metadata": {
        "id": "XVcSMVfby845"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "epochs = 10\n",
        "\n",
        "# offset input is required due to how embeddings are loaded in\n",
        "offset_input = torch.tensor([0])\n",
        "\n",
        "for _ in range(epochs):\n",
        "  \n",
        "  tr_count = 0\n",
        "  sst_tr_acc = 0\n",
        "  sst_tr_loss = 0\n",
        "  cola_tr_acc = 0\n",
        "  cola_tr_loss = 0\n",
        "  for sample in tqdm(train_loader):\n",
        "    # Note can also set the data to a decide (cuda)\n",
        "    model.zero_grad()\n",
        "    # print(type(sample.sst_input_inds))\n",
        "    # print(torch.tensor(sample.sst_input_inds))\n",
        "    # print(torch.tensor(sample.sst_input_inds).type(torch.long))\n",
        "    model_sst_inp = torch.tensor(sample.sst_input_inds)\n",
        "    print(model_sst_inp.shape)\n",
        "    model_cola_inp = torch.tensor(sample.cola_input_inds)\n",
        "    print(model_cola_inp.shape)\n",
        "    y1_pred = model(model_sst_inp, 0, None).reshape(-1, 2)\n",
        "    y2_pred = model(model_cola_inp, 1, None).reshape(-1, 2)\n",
        "    sample_sst_target = torch.tensor(sample.sst_target).type(torch.long).reshape(-1)\n",
        "    sample_cola_target = torch.tensor(sample.cola_target).type(torch.long).reshape(-1)\n",
        "    print(y1_pred.shape)\n",
        "    print(sample_sst_target.shape)\n",
        "    sst_loss = criterion(y1_pred, sample_sst_target)\n",
        "    cola_loss = criterion(y2_pred, sample_cola_target)\n",
        "    # multitask loss\n",
        "    loss = sst_loss + cola_loss\n",
        "    loss.backward()\n",
        "\n",
        "    # intermediate variabels stores embedding of x and computes\n",
        "    # m_output.grad w.r.t. this and calculate the MSE of the m_output.grad(emb_x)\n",
        "    # norm and 1.0 and use relu on; before you do the square, pass it through a relu\n",
        "    # so everything les than 1.0 wont be counted to the square.\n",
        "    # MSE (m_output.grad(x), 1.0) calulate gradient of M output w.r.t. x's embedding space\n",
        "    # To avoid overfitting, calculating regularization term can use a varied version of x\n",
        "    # instead of the original (e.g. add gaussian noise around embeddings of x); can\n",
        "    # also minimize discrepancy on the two for robustness of model\n",
        "    # Lipschitz-regularized loss\n",
        "    optimizer.step()\n",
        "    sst_tr_acc += (torch.eq(sample_sst_target,y1_pred.argmax(1))).sum().item()\n",
        "    cola_tr_acc += (torch.eq(sample_cola_target, y2_pred.argmax(1))).sum().item()\n",
        "    sst_tr_loss += sst_loss.item()\n",
        "    cola_tr_loss += cola_loss.item()\n",
        "    tr_count += y1_pred.shape[0]\n",
        "    \n",
        "  print(\"SST Train accuracy: {}, CoLA Train accuracy: {}, SST Train loss: {}, CoLA Train loss: {}\".format(sst_tr_acc/tr_count, cola_tr_acc/tr_count, sst_tr_loss/tr_count, cola_tr_loss/tr_count))\n",
        "  val_sst_acc, val_sst_loss, val_cola_acc, val_cola_loss = eval_metrics(model, val_loader, criterion)\n",
        "  print(\"SST Val accuracy: {}, CoLA Val accuracy: {}, SST Val loss: {}, CoLA Val loss: {} \".format(val_sst_acc, val_cola_acc, val_sst_loss, val_cola_loss))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 709
        },
        "id": "pT08aZRox_a7",
        "outputId": "c7d997c5-7bc0-4e99-9b28-5efaf9cbd9b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/962 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 32])\n",
            "torch.Size([8, 32])\n",
            "inside forward\n",
            "torch.Size([8, 32])\n",
            "torch.Size([8, 300])\n",
            "torch.Size([8, 300])\n",
            "done with dropout\n",
            "inside forward\n",
            "torch.Size([8, 32])\n",
            "torch.Size([8, 300])\n",
            "torch.Size([8, 300])\n",
            "done with dropout\n",
            "torch.Size([1, 2])\n",
            "torch.Size([8])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-9201bdc9228f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my1_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_sst_target\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0msst_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my1_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_sst_target\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0mcola_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my2_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_cola_target\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;31m# multitask loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1163\u001b[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[1;32m   1164\u001b[0m                                \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1165\u001b[0;31m                                label_smoothing=self.label_smoothing)\n\u001b[0m\u001b[1;32m   1166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   2994\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2995\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2996\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_smoothing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2997\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2998\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Expected input batch_size (1) to match target batch_size (8)."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_sst_acc, test_sst_loss, test_cola_acc, test_cola_loss = eval_metrics(model, val_loader, criterion)\n",
        "print(\"SST Test accuracy: {}, CoLA Val accuracy: {}, SST Val loss: {}, CoLA Val loss: {} \".format(test_sst_acc, test_cola_acc, test_sst_loss, test_cola_loss))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JeT2uQEIhS8n",
        "outputId": "68fec03a-3579-47f7-99f0-bc06ba075146"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SST Test accuracy: 0.6063084112149533, CoLA Val accuracy: 0.6273364485981309, SST Val loss: 0.23364485800266266, CoLA Val loss: 0.0034176348708570004 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TODO: Adversarial attacks and Robustness Evaluation"
      ],
      "metadata": {
        "id": "NlCTcEOOSgcV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def attack():\n",
        "  pass"
      ],
      "metadata": {
        "id": "fLH_bFIASkWE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}